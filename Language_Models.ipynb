{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPodKLDpHGZwewPjCG61SCx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9LQ0-GenAI/blob/main/Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Modeling\n",
        "Language modeling in natural language processing (NLP) plays a pivotal role in the development of intelligent systems that can understand and generate human language. Essentially, a language model aims to predict the likelihood of a sequence of words or the probability of the next word given a specific context. By capturing the underlying structure and patterns in textual data, language models facilitate various NLP tasks, such as machine translation, text summarization, sentiment analysis, and conversational AI.\n"
      ],
      "metadata": {
        "id": "iEgclihJUPC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Probabilistic Language Modeling with n-grams\n",
        "Probabilistic language modeling using n-grams is a fundamental approach in NLP that leverages the statistical properties of text to predict word sequences. An n-gram model represents text as contiguous sequences of n words, where the context for predicting the next word is limited to the previous n-1 words. For instance, a bigram (n=2) model restricts the context to a single preceding word, while a trigram (n=3) model considers the two preceding words.\n",
        "N-gram models estimate the probabilities of word sequences by calculating their frequency in a given corpus. They utilize the Markov assumption, which states that the probability of the next word depends only on the preceding n-1 words, thus simplifying computation.\n",
        "\n",
        "Despite their simplicity, n-gram models have been widely used in various NLP tasks, such as speech recognition, machine translation, and text generation. However, they have limitations, including data sparsity and the inability to capture long-range dependencies in text. The emergence of more sophisticated techniques like deep learning-based language models has shifted the focus, but n-gram models still hold relevance as a foundation for understanding language modeling and its development.\n",
        "\n",
        "In this session, we will explore n-grams-based language modeling.\n"
      ],
      "metadata": {
        "id": "xjh4t0WZUk9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example: A Jane Austin Novel\n",
        "We will explore ‘next word prediction’ and ‘text generation’ based on the Jane Austin novel ‘Sense and Sensibility’. Let’s look first at all the books that are available.\n"
      ],
      "metadata": {
        "id": "Ee4joPHvVqyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View Books and Download"
      ],
      "metadata": {
        "id": "8fFPzF_QbRcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade jax jaxlib\n",
        "#!pip install numpy==1.24.3"
      ],
      "metadata": {
        "id": "WdTVDOGpiPRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select Runtime-Restart session after installing jax**"
      ],
      "metadata": {
        "id": "Gr0IWggbwbhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')  # Make sure the Gutenberg corpus is downloaded\n",
        "from nltk.corpus import gutenberg"
      ],
      "metadata": {
        "id": "mUhVAAMI4VXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List available texts in the Gutenberg corpus\n",
        "print(gutenberg.fileids())"
      ],
      "metadata": {
        "id": "3f6uY6EUWHEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load \"Sense and Sensibility\" text\n",
        "sas = gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Print the first 500 characters of \"Sense and Sensibility\"\n",
        "print(sas[:5000])\n"
      ],
      "metadata": {
        "id": "ZWmkgwQIWnu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##n-gram Model for Next Word Prediction and Text Generation.\n",
        "\n",
        "Following are the key steps.\n",
        "\n",
        "* **Step 1: Preprocess the Text**\n",
        "\n",
        "Start by tokenizing the text.\n",
        "\n",
        "* **Step 2: Build the N-gram Model**\n",
        "\n",
        "Create a trigram model, which will be used for predicting the next word based on the previous two words.\n",
        "\n",
        "* **Step 3: Next Word Prediction**\n",
        "\n",
        "Write a function that takes two words as input and predicts the most probable next word.\n",
        "\n",
        "* **Step 4: Text Generation**\n",
        "\n",
        "Using the trigram model, generate text by iteratively predicting the next word."
      ],
      "metadata": {
        "id": "W8cLDMiEZgot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, ngrams\n",
        "from collections import defaultdict, Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(sas.lower())  # Convert to lower case\n",
        "\n",
        "# Generate trigrams from the tokens\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "trigram_freq = defaultdict(Counter)\n",
        "\n",
        "# Populate the frequencies of trigrams\n",
        "for w1, w2, w3 in trigrams:\n",
        "    trigram_freq[(w1, w2)][w3] += 1\n",
        "\n",
        "# Function to predict the next word\n",
        "def predict_next_word(w1, w2):\n",
        "    if (w1, w2) in trigram_freq:\n",
        "        # Get the most common next word for the given bigram (w1, w2)\n",
        "        return trigram_freq[(w1, w2)].most_common(1)[0][0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(start_words, num_words):\n",
        "    if len(start_words) < 2:\n",
        "        return \"Please provide at least two starting words.\"\n",
        "\n",
        "    generated_words = list(start_words)\n",
        "    for _ in range(num_words):\n",
        "        next_word = predict_next_word(generated_words[-2], generated_words[-1])\n",
        "        if next_word is None:\n",
        "            break  # Break if no next word is found\n",
        "        generated_words.append(next_word)\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "0Vxf3sp1ZhDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams"
      ],
      "metadata": {
        "id": "18X1KBmxw2K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trigrams)"
      ],
      "metadata": {
        "id": "2iG0P4IwUdo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Results"
      ],
      "metadata": {
        "id": "y9xSv4lVb0FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the prediction function\n",
        "print(\"Next word:\", predict_next_word('was', 'the'))"
      ],
      "metadata": {
        "id": "uuylj7PCafnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the text generation function\n",
        "start_words = (\"the\", \"more\")\n",
        "generate_text(start_words, 100)"
      ],
      "metadata": {
        "id": "94V121kEaNiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improving Text Generation\n",
        "The approach of  picking the most frequent next word in text generation results in text repetition and uninteresting text.\n",
        "\n",
        "We will modify the `predict_next_word` function to choose the next word based on a probability distribution rather than just picking the most frequent next word. This way, the selection will still favor more likely words but won't always select the same word every time.\n"
      ],
      "metadata": {
        "id": "qUmqOYkycYnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Function to predict the next word with randomness\n",
        "def predict_next_word(w1, w2):\n",
        "    if (w1, w2) in trigram_freq:\n",
        "        next_words = list(trigram_freq[(w1, w2)].elements())\n",
        "        return random.choice(next_words) if next_words else None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to generate text with randomness\n",
        "def generate_text(start_words, num_words):\n",
        "    if len(start_words) < 2:\n",
        "        return \"Please provide at least two starting words.\"\n",
        "\n",
        "    generated_words = list(start_words)\n",
        "    for _ in range(num_words):\n",
        "        next_word = predict_next_word(generated_words[-2], generated_words[-1])\n",
        "        if next_word is None:\n",
        "            break  # Break if no next word is found\n",
        "        generated_words.append(next_word)\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "1njN8TtucZMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the text generation function\n",
        "start_words = (\"it\", \"was\")\n",
        "generate_text(start_words, 500)"
      ],
      "metadata": {
        "id": "vJRIisIOdMZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your turn\n",
        "\n",
        "Create next word prediction and text generation using n-grams based on the novel Moby Dick, by Herman Melville. You can access it with `melville-moby_dick.txt`."
      ],
      "metadata": {
        "id": "x_fhW1xr2ijg"
      }
    }
  ]
}