{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+HlXATrnadgcqfUfBYxhs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9LQ0-GenAI/blob/main/8_3_simple_multi_agent_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Multi-Agent System\n",
        "In this tutorial we will put together a simple multi-agent system designed to research AI.\n",
        "\n",
        "We will introduce a new tool for this system ... internet search via DuckDuckGo (a search engine).\n",
        "\n",
        "Note, we will use the bigger version of Falcon (10bil parameters) because otherwise the results will be ... underwhelming.\n",
        "\n",
        "We start with the usual code we have seen all session:"
      ],
      "metadata": {
        "id": "JTsGvsQdj7YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes duckduckgo-search\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from duckduckgo_search import DDGS\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# If you haven't already added the secret in Colab, here's how:\n",
        "# Runtime > Secrets > Set 'HF_TOKEN' with your Hugging Face API token\n",
        "\n",
        "hf_api_token = userdata.get('HF_TOKEN')  # Make sure the secret is set in Colab\n",
        "\n",
        "# Set the token if it's retrieved correctly, else print an error\n",
        "if hf_api_token:\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_api_token\n",
        "else:\n",
        "    print(\"Hugging Face API Token is missing. Please set it in Colab Secrets.\")\n",
        "\n",
        "# Load a small instruction-following model\n",
        "\n",
        "# model_id = \"tiiuae/falcon-7b-instruct\" # faster but less accurate\n",
        "model_id = \"tiiuae/Falcon3-10B-Instruct\" # this will be quite slow\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    offload_buffers=True, # offloads stuff like attention scores to CPU to free up GPU\n",
        "    low_cpu_mem_usage=True\n",
        "    # low_cpu_mem_usage loads only parts of the module into RAM at a given moment in\n",
        "    # time, thus meaning we don't need to fill up RAM with the whole model\n",
        ")\n",
        "\n",
        "# we also define a pipeline as well will be repeatedly querying the LLM model\n",
        "pipe = pipeline(\n",
        "       \"text-generation\",\n",
        "       model=model,\n",
        "       tokenizer=tokenizer,\n",
        "       device_map=\"auto\",\n",
        "       torch_dtype=torch.float16\n",
        "   )"
      ],
      "metadata": {
        "id": "heZBlBPyXx4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define our tool - DuckDuckGo search (via the DDGS function of the DuckDuckGo-Search Python package)"
      ],
      "metadata": {
        "id": "uows3bWGl6Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DuckDuckGo tool\n",
        "def search_duckduckgo(query): # pass user query to DuckDuckGo\n",
        "    with DDGS() as ddgs:\n",
        "\n",
        "        # query using the user query and get top 3 results\n",
        "        results = ddgs.text(query, max_results=3)\n",
        "\n",
        "    # combine the top three results (just the meta description)\n",
        "    return \"\\n\".join([r[\"body\"] for r in results])"
      ],
      "metadata": {
        "id": "m8eXZaIRdrxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our tool setup we can now create our agent. We will follow the OOP-style approach of creating an object from a class template (we discussed this in ADA)."
      ],
      "metadata": {
        "id": "buuivwjKmdsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResearchAgent:\n",
        "    # rules to apply when instantiating an object of this class\n",
        "    def __init__(self, name, goal, llm, tools):\n",
        "        self.name = name # name of this object/agent\n",
        "        self.goal = goal # the agent's goal\n",
        "        self.llm = llm # the LLM we installed\n",
        "        self.tools = tools # the tools it can use (just DuckDuckGo in this case)\n",
        "\n",
        "    def run_task(self, query):\n",
        "        # extract context by using the search tool\n",
        "        context = self.tools[\"search\"](query)\n",
        "\n",
        "        # prompt template\n",
        "        prompt = f\"\"\"\n",
        "          You are a {self.name}.\n",
        "          Your goal is: {self.goal}\n",
        "          Using the following information from web search:\n",
        "          {context}\n",
        "          Write a comprehensive and professional report on the goal.\n",
        "          Please write no more than 200 words.\n",
        "          Do not repeat yourself.\n",
        "        \"\"\"\n",
        "\n",
        "        # rules for how to create the response\n",
        "        response = self.llm(prompt, # the prompt above\n",
        "                            # only return the answer not the prompt (full text)\n",
        "                            return_full_text=False,\n",
        "                            # maximum number of tokens to return\n",
        "                            max_new_tokens=256,\n",
        "                            # sample when decoding the response (tokens -> to text)\n",
        "                            # this allows the decoder to be a bit more creative when\n",
        "                            # generating the output by referring to the whole probability\n",
        "                            # distribution of the output rather than just the most likely.\n",
        "                            do_sample=True,\n",
        "                            # temperature controls how much creativity the LLM can have\n",
        "                            # higher is more creativity\n",
        "                            # [0] means return just the main response\n",
        "                            # extract from this the \"generated_text\" item\n",
        "                            temperature=0.8)[0][\"generated_text\"]\n",
        "        return response"
      ],
      "metadata": {
        "id": "fAv_OAoLeFRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have set up the Class template we can create an object of this type."
      ],
      "metadata": {
        "id": "KtVQ_hCuoJcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up agent and tools\n",
        "\n",
        "# first create the tools list - just one tool\n",
        "tools = {\"search\": search_duckduckgo}\n",
        "\n",
        "# now create an object of the ResearchAgent Class template. As per the __init__ rules,\n",
        "# it must have a name, a goal, an LLM connection (our pipeline) and the toolset.\n",
        "agent1 = ResearchAgent(\n",
        "    name=\"AI Trend Analyst\",\n",
        "    goal=\"Summarize the top 2024 AI breakthroughs\",\n",
        "    llm=pipe,\n",
        "    tools=tools\n",
        ")"
      ],
      "metadata": {
        "id": "TfNCJtGjdUwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then let's put the agent to work!"
      ],
      "metadata": {
        "id": "qk6wgR_hoga7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the agent\n",
        "agent1_report = agent1.run_task(\"AI breakthroughs in 2024\")\n",
        "print(\"=== FINAL REPORT ===\\n\")\n",
        "print(agent1_report)"
      ],
      "metadata": {
        "id": "r-9K2YoKeTmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming you used the (slow) 10bil parameter model the response should be quite good! Now let's build some more agents:"
      ],
      "metadata": {
        "id": "bFR43gK3oqeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent2 = ResearchAgent(\n",
        "    name=\"AI Data Scientist\",\n",
        "    goal=\"Identify breakthrough AI technologies and research papers published in 2024\",\n",
        "    llm=pipe,\n",
        "    tools={\"search\": search_duckduckgo}\n",
        ")\n",
        "\n",
        "agent3 = ResearchAgent(\n",
        "    name=\"AI Ethics Officer\",\n",
        "    goal=\"Review ethical concerns and AI regulations that emerged in 2024\",\n",
        "    llm=pipe,\n",
        "    tools={\"search\": search_duckduckgo}\n",
        ")"
      ],
      "metadata": {
        "id": "wYPIuJlHgcrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have two more agents doing slightly different searches to support slightly different goals. We should get three unique reports!\n",
        "\n",
        "However, reading three reports sounds very boring. What we really want is another agent that can summarise all three individual reports into just one report. Because AI should help us be more lazy:"
      ],
      "metadata": {
        "id": "SyJ1Q_pmozhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SummariserAgent:\n",
        "    def __init__(self, name, role, llm):\n",
        "        self.name = name\n",
        "        self.role = role\n",
        "        self.llm = llm\n",
        "\n",
        "    def summarise_reports(self, reports):\n",
        "        # Combine the reports from other agents\n",
        "        combined_reports = \"\\n\".join(reports)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "          Your goal is to summarize the following reports into a cohesive, professional summary:\n",
        "\n",
        "          {combined_reports}\n",
        "\n",
        "          The report should be at least three paragraphs in length.\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.llm(prompt,\n",
        "                              return_full_text=False,\n",
        "                              # double the amount of tokens 256 for the research agents\n",
        "                              # 512 for our summariser\n",
        "                              max_new_tokens=512,\n",
        "                              do_sample=True,\n",
        "                              temperature=0.8)[0][\"generated_text\"]\n",
        "        return response"
      ],
      "metadata": {
        "id": "Tk7axDAYgzb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And again we can create an object of this Class template:"
      ],
      "metadata": {
        "id": "lWccY4ldpw1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summariser = SummariserAgent(\n",
        "    name=\"AI Research Summarizer\",\n",
        "    role=\"Senior AI Research Analyst\",\n",
        "    llm=pipe\n",
        ")"
      ],
      "metadata": {
        "id": "acCABtlNg7_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can call research agents #2 and #3:"
      ],
      "metadata": {
        "id": "fpwSQBY7p4KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache() # empty the cache - rolling memory in RAM to avoid overload\n",
        "\n",
        "# hashed out as we ran this earlier\n",
        "\n",
        "# Task 1: AI Researcher\n",
        "#agent1_report = agent1.run_task(\"AI breakthroughs in 2024\")\n",
        "#print(f\"\\n[{agent1.name}] Report:\\n{agent1_report}\\n\")\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# Task 2: AI Data Scientist\n",
        "agent2_report = agent2.run_task(\"AI research papers in 2024\")\n",
        "print(f\"\\n[{agent2.name}] Report:\\n{agent2_report}\\n\") # print report\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Task 3: AI Ethics Officer\n",
        "agent3_report = agent3.run_task(\"AI ethics and regulations in 2024\")\n",
        "print(f\"\\n[{agent3.name}] Report:\\n{agent3_report}\\n\") # print report\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UV6WLLNzhCER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, decent looking work (assuming 10bil). Let's summarise!"
      ],
      "metadata": {
        "id": "BWQuvp5pqRHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§  Summarise all the reports\n",
        "reports = [agent1_report, agent2_report, agent3_report]\n",
        "final_report = summariser.summarise_reports(reports)\n",
        "\n",
        "print(f\"\\nSummary Report:\\n{final_report}\\n\")\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UWTYkhCz_kYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bit slow, but a good overall summary!\n",
        "\n",
        "### __TASK__:\n",
        "Can you create another agent that checks the work of the researchers and asks them to rewrite if the response is not good? What would this agent class look like? How could we create the flow of work to complete these checks?\n",
        "\n",
        "You don't necessarily need to write this code in full but think about what it would look like."
      ],
      "metadata": {
        "id": "JihtpVVQqXUj"
      }
    }
  ]
}