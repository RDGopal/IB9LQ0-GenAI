{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPGP2+OHu1Kc2ojphQN3zru",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9LQ0-GenAI/blob/main/Fine_tuning_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tuning an LLM\n",
        "Fine-tuning refers to the process of taking a pre-trained Large Language Model (LLM) and training it further on a smaller, specialized dataset to adapt it for a specific task or domain. By exposing them to domain-specific data, we can enhance their ability to tackle specialized tasks—whether it’s for classification, writing legal contracts, or processing medical diagnoses with greater accuracy.\n",
        "\n",
        "\n",
        "##Full Fine-tuning\n",
        "Full fine-tuning involves taking a pre-trained language model and updating all of its trainable parameters (weights and biases) on your new, task-specific dataset. The entire model architecture remains the same; only the values of the weights are adjusted during the training process to better perform on the downstream task.\n",
        "\n",
        "While full fine-tuning is a straightforward approach, it can be computationally expensive (especially for large models) and may lead to overfitting on small datasets.\n",
        "\n",
        "\n",
        "##Parameter-Efficient Fine-Tuning (PEFT)\n",
        "These methods aim to achieve performance comparable to full fine-tuning while only training a small fraction of the model's parameters. This significantly reduces computational cost and storage requirements. Some popular PEFT techniques include:\n",
        "\n",
        "* Low-Rank Adaptation (LoRA): Introduces small, low-rank matrices alongside the original weights. Only these low-rank matrices are trained, while the original pre-trained weights are kept frozen.\n",
        "\n",
        "\n",
        "* QLoRA (Quantization-aware Low-Rank Adaptation): Combines quantization (reducing the precision of weights) with LoRA to further reduce memory footprint during training.\n",
        "\n",
        "* Instruction Tuning: This is a specific type of fine-tuning where the model is trained on a dataset of tasks described in natural language instructions. The goal is to make the model better at following instructions and generalizing to new tasks it hasn't seen explicitly during fine-tuning. This often involves formatting the training data as (instruction, input, output) triplets.\n",
        "\n",
        "* Reinforcement Learning from Human Feedback (RLHF): While not strictly a fine-tuning method in the traditional supervised sense, RLHF is a crucial technique for aligning large language models with human preferences. It typically involves several stages, including supervised fine-tuning followed by training a reward model based on human comparisons of different model outputs, and finally using reinforcement learning to optimize the language model to maximize this reward.\n"
      ],
      "metadata": {
        "id": "tFy2jQVBIsHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full Fine-tuning\n",
        "We will first conduct full fine-tuning and then explore PEFT."
      ],
      "metadata": {
        "id": "v5CYU9R8jG7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install Necessary Libraries\n",
        "* transformers: For working with pre-trained models and datasets.\n",
        "* datasets: For easily creating and managing small datasets."
      ],
      "metadata": {
        "id": "XngtYNR5Kopd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg61OCgEGXqf"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, DatasetDict # Import DatasetDict here\n",
        "import random\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load a Small Sample of the Dataset:\n",
        "We will load the emotion dataset and then take a small random sample."
      ],
      "metadata": {
        "id": "_9-gWpI7Gzz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --upgrade\n",
        "from datasets import load_dataset, DatasetDict # Import DatasetDict here\n",
        "import random\n",
        "dataset_name = \"emotion\"\n",
        "full_dataset = load_dataset(dataset_name)"
      ],
      "metadata": {
        "id": "nibxD_QTH_pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset"
      ],
      "metadata": {
        "id": "K1s4ww-HIZVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "oc-0rkLmAZng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(str(full_dataset['train'][:5])))"
      ],
      "metadata": {
        "id": "WbPS0aI2AI2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Take 100 rows of data"
      ],
      "metadata": {
        "id": "Q5YqCkS3L8Yc"
      }
    },
    {
      "source": [
        "# Combine train, validation, and test sets for sampling\n",
        "from datasets import concatenate_datasets # Import concatenate_datasets here\n",
        "\n",
        "combined_dataset = full_dataset[\"train\"].shuffle(seed=42).select(range(50))  # Take 50 from train\n",
        "# Convert to DatasetDict and give it a key name, like \"train\"\n",
        "combined_dataset = DatasetDict({\"train\": combined_dataset})\n",
        "\n",
        "# Concatenate validation set to 'train'\n",
        "validation_subset = full_dataset[\"validation\"].shuffle(seed=42).select(range(25)).cast(combined_dataset[\"train\"].features)\n",
        "# Instead of using concatenate_datasets on the Dataset, update the DatasetDict\n",
        "combined_dataset[\"train\"] = concatenate_datasets([combined_dataset[\"train\"], validation_subset]) # Use concatenate_datasets directly\n",
        "\n",
        "# Concatenate test set to 'train'\n",
        "test_subset = full_dataset[\"test\"].shuffle(seed=42).select(range(25)).cast(combined_dataset[\"train\"].features)\n",
        "# Instead of using concatenate_datasets on the Dataset, update the DatasetDict\n",
        "combined_dataset[\"train\"] = concatenate_datasets([combined_dataset[\"train\"], test_subset]) # Use concatenate_datasets directly\n",
        "\n",
        "sampled_dataset = combined_dataset[\"train\"].shuffle(seed=42).select(range(100)) # Final shuffled sample of 100"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jltpuDfZIwbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(f\"Number of examples in the sampled dataset: {len(sampled_dataset)}\"))\n",
        "display(Markdown(str(sampled_dataset[:5])))"
      ],
      "metadata": {
        "id": "STgttiVKDOPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the dataset to a Pandas DataFrame\n",
        "df = sampled_dataset.to_pandas()\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "B4MHQKDwLzqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load a Tiny Pre-trained Language Model and Tokenizer"
      ],
      "metadata": {
        "id": "sadbx0IZI5-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id"
      ],
      "metadata": {
        "id": "BqZoVnuXI7o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Architecture of the Base Model"
      ],
      "metadata": {
        "id": "6B7FOR142MBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print layer names\n",
        "for name, param in model.named_parameters():\n",
        "    print(name)"
      ],
      "metadata": {
        "id": "Z5Lq1KaK2MW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Number of Parameters in the Base Model"
      ],
      "metadata": {
        "id": "lcxuc-As2VYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# Print the result\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ],
      "metadata": {
        "id": "8Bak3o1N2bgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess the Dataset:\n",
        "Now, we need to format our data for the language model. We format our data as:\n",
        "\n",
        "emotion: `[emotion_label]` text: `[text]`\n",
        "\n",
        "The model will learn to associate the emotion label with the style of the text."
      ],
      "metadata": {
        "id": "YvkpI8POJEqu"
      }
    },
    {
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [f\"emotion: {label} text: {text}{tokenizer.eos_token}\" for text, label in zip(examples['text'], examples['label'])]\n",
        "    # Tokenize inputs and add 'labels' key\n",
        "    tokenized_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=128)\n",
        "    # Shift labels to align with model's expected input format (Causal LM)\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_dataset = sampled_dataset.map(preprocess_function, batched=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pO55pQfDKCRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##View Prepared Data"
      ],
      "metadata": {
        "id": "uYfCDgtAN09b"
      }
    },
    {
      "source": [
        "for i in range(5):  # Print the first 5 examples\n",
        "       example = tokenized_dataset[i]\n",
        "       decoded_text = tokenizer.decode(example['input_ids'])\n",
        "       print(f\"Example {i + 1}:\")\n",
        "       print(decoded_text)\n",
        "       print(example)  # Print the full dictionary for the example\n",
        "       print(\"-\" * 20)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FrCgjuv6NphZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Training Arguments"
      ],
      "metadata": {
        "id": "jX_bogk4JRG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_emotion_model\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=10,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    save_strategy=\"epoch\"\n",
        ")"
      ],
      "metadata": {
        "id": "5uo4VT3LJVgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the Trainer"
      ],
      "metadata": {
        "id": "orXo32HSJZIu"
      }
    },
    {
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=lambda data: tokenizer.pad(data, padding='max_length', max_length=128, return_tensors='pt')\n",
        "    # Call the pad method with appropriate arguments within a lambda function\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bJyd3tHgJ4V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tune the Model"
      ],
      "metadata": {
        "id": "xhM8LVIJJhyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "GBNnL9fWJle1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test the Fine-tuned Model"
      ],
      "metadata": {
        "id": "976PlUv4JpA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_to_generate = \"happy\"\n",
        "prompt = f\"emotion: {emotion_to_generate} text:\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "generated_text = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "display(Markdown(f\"Prompt: {prompt}\"))\n",
        "display(Markdown(f\"Fine-tuned Model (Emotion: {emotion_to_generate}) Response: {generated_text}\"))"
      ],
      "metadata": {
        "id": "R-aJxPCuKMcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameter-Efficient Fine-Tuning (PEFT)\n",
        "We will now explore PEFT. We will continue to use the same dataset and the same LLM.\n"
      ],
      "metadata": {
        "id": "9hdPAY-BM5qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate peft"
      ],
      "metadata": {
        "id": "Qc1QfCrmOA-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "import torch"
      ],
      "metadata": {
        "id": "bf69OqSeOA7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps including loading and preparing the data, loading the LLM and Tokenizer remain the same as before.**"
      ],
      "metadata": {
        "id": "WKDAyH6vPdFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define Training Arguments"
      ],
      "metadata": {
        "id": "keqYThnMj4Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\", # Specify the task type\n",
        "    target_modules=[\"attn.c_attn\", \"attn.c_proj\"], # Adjust based on the model architecture\n",
        ")\n",
        "\n",
        "# Wrap the base model with the LoRA adapters\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters() # See how many parameters are trainable"
      ],
      "metadata": {
        "id": "1Oj1KtCsOXzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create the Trainer"
      ],
      "metadata": {
        "id": "7QsvzaEuj93r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_fine_tuned_emotion_model\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=30,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    learning_rate=2e-4, # LoRA often benefits from slightly higher learning rates\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    save_strategy=\"epoch\"\n",
        ")"
      ],
      "metadata": {
        "id": "0udXu0zvOg-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=lambda data: tokenizer.pad(data, padding='max_length', max_length=128, return_tensors='pt')\n",
        "    # Call the pad method with appropriate arguments within a lambda function\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "M44crRE5OnHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the Model"
      ],
      "metadata": {
        "id": "xwiYSTpxkHx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "drtGI_s4OujF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test the Fine-tuned Model"
      ],
      "metadata": {
        "id": "IKl5gsDTkNT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_to_generate = \"joy\"\n",
        "prompt = f\"emotion: {emotion_to_generate} text:\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "output = model.generate(input_ids, max_length=500, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "generated_text = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"LoRA Fine-tuned Model (Emotion: {emotion_to_generate}) Response: {generated_text}\")\n",
        "\n",
        "emotion_to_generate_2 = \"sadness\"\n",
        "prompt_2 = f\"emotion: {emotion_to_generate_2} text:\"\n",
        "input_ids_2 = tokenizer.encode(prompt_2, return_tensors=\"pt\").to(model.device)\n",
        "output_2 = model.generate(input_ids_2, max_length=500, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "generated_text_2 = tokenizer.decode(output_2[:, input_ids_2.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"\\nPrompt: {prompt_2}\")\n",
        "print(f\"LoRA Fine-tuned Model (Emotion: {emotion_to_generate_2}) Response: {generated_text_2}\")\n",
        "\n",
        "# You can also test the original model for comparison\n",
        "original_model = AutoModelForCausalLM.from_pretrained(model_name).to(model.device)\n",
        "original_output = original_model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "original_response = tokenizer.decode(original_output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "print(f\"\\nOriginal Model Response (with 'joy' prompt): {original_response}\")"
      ],
      "metadata": {
        "id": "GgWlQsK9PCZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Your turn\n",
        "1. Fine-tune based on the data `AI_Human_Review.csv`. The first column contains AI written text and the second column contains the corresponding Human written text. The objective of fine-tuning is to take AI written text and output it's human written equivalent text. Fine-tune based on a sample of 100 data points.\n",
        "2. Fine-tune based on the data `fakenews100.csv`. The first column contains text and the second column indicates whether it is fake or not. The objective of fine-tuning is to take input text and identify if it is fake news. Fine-tune based on a sample 100 data points."
      ],
      "metadata": {
        "id": "RQ7HrqmS5OIJ"
      }
    }
  ]
}