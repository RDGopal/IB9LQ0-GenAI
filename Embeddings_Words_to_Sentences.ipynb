{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNXBbxdw2DuPRCvS+kOIrm7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9LQ0-GenAI/blob/main/Embeddings_Words_to_Sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Embeddings\n",
        "Embeddings are dense vector representations of text that capture semantic meaning. They transform words, sentences, or even documents into numerical forms that machines can process, preserving relationships like similarity or context. Embeddings are key to modern Natural Language Processing (NLP) and Generative AI because they:\n",
        "\n",
        "Enable models to understand linguistic patterns and semantics.\n",
        "Power tasks like text classification, sentiment analysis, machine translation, and text generation.\n",
        "Allow efficient computation of relationships (e.g., similarity) between pieces of text.\n",
        "In this exercise, we'll explore three types of embeddings using a sample text:\n",
        "\n",
        "Word Embeddings: Static vectors for individual words (e.g., GloVe).\n",
        "Sentence Embeddings: Vectors for entire sentences (e.g., via sentence-transformers).\n",
        "Contextual Embeddings: Dynamic vectors that vary by context (e.g., BERT)."
      ],
      "metadata": {
        "id": "T59rcT2wEvUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install gensim matplotlib scikit-learn sentence-transformers transformers"
      ],
      "metadata": {
        "id": "FfL4c2bGFdx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select Runtime-Restart session after installing the above packages**"
      ],
      "metadata": {
        "id": "CLIjt55gy_Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "2WXG7sLnQOOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sample Text\n",
        "We will start with this text for word and sentence embeddings:"
      ],
      "metadata": {
        "id": "guQG79IhE4hs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8AUBCp8EOc8"
      },
      "outputs": [],
      "source": [
        "text = \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"Natural language processing is a subfield of linguistics.\",\n",
        "    \"It involves computer science and artificial intelligence.\",\n",
        "    \"The goal is to enable computers to understand human language.\",\n",
        "    \"Machine learning is a key component of NLP.\",\n",
        "    \"Deep learning has revolutionized the field.\",\n",
        "    \"Transformers are a type of deep learning model.\",\n",
        "    \"BERT is a popular transformer model for NLP tasks.\"\n",
        "]"
      ],
      "metadata": {
        "id": "ddNIjaLqETul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_context = [\n",
        "    \"I went to the bank to deposit money.\",\n",
        "    \"The river bank was flooded.\"\n",
        "]"
      ],
      "metadata": {
        "id": "i9uKLUWsEhD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Embeddings\n",
        "Word embeddings map individual words to fixed-size vectors based on their meanings, learned from large corpora. Popular models include Word2Vec and GloVe. Here, we'll use GloVe (Global Vectors), which captures global statistical information about word co-occurrences."
      ],
      "metadata": {
        "id": "Ev4E-SsEFJlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import gensim.downloader\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load pre-trained GloVe model (50-dimensional vectors)\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "# Select words from our text to analyze\n",
        "words = [\"language\", \"computer\", \"human\", \"machine\", \"intelligence\", \"artificial\", \"data\", \"process\", \"analyze\"]\n",
        "\n",
        "# Get their vectors\n",
        "vectors = [word_vectors[word] for word in words]\n",
        "\n",
        "# Compute cosine similarity between two words\n",
        "similarity = word_vectors.similarity(\"language\", \"computer\")\n",
        "print(f\"Similarity between 'language' and 'computer': {similarity:.4f}\")"
      ],
      "metadata": {
        "id": "3Jo0uPwKEnrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize"
      ],
      "metadata": {
        "id": "lDMXWeZNH5C1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The similarity score (e.g., between \"language\" and \"computer\") will be a value between -1 and 1, with higher values indicating greater semantic similarity.\n",
        "\n",
        "The t-SNE plot projects 50-dimensional vectors into 2D space. Words with similar meanings (e.g., \"computer\" and \"machine\") should appear closer together, while unrelated words (e.g., \"human\" and \"data\") may be farther apart."
      ],
      "metadata": {
        "id": "o5GeVPcwImUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce dimensions to 2D using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42,perplexity=5)\n",
        "vectors_2d = tsne.fit_transform(np.array(vectors))\n",
        "\n",
        "# Plot the embeddings\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i, word in enumerate(words):\n",
        "    plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1])\n",
        "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), fontsize=12)\n",
        "plt.title(\"Word Embeddings Visualization (GloVe)\", fontsize=14)\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sv_VtMwTH7Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Missing Context\n",
        "\n",
        "Consider the following sentences:\n",
        "\n",
        "    \"I went to the bank to deposit money.\"\n",
        "    \"The river bank was flooded.\"\n",
        "Let's create a vocabulary from the two sentences, get then embeddings and plot them.\n"
      ],
      "metadata": {
        "id": "Z2JgxF4EJGzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "QqIHMWomKKlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"I went to the bank to deposit money.\",\n",
        "    \"The river bank was flooded.\"\n",
        "]\n",
        "all_text = \" \".join(sentences)\n",
        "all_text = all_text.lower()\n",
        "words = word_tokenize(all_text)\n",
        "words"
      ],
      "metadata": {
        "id": "ngssTTfKLiBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get embeddings and plot them"
      ],
      "metadata": {
        "id": "DdUswQqkNHK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get their vectors\n",
        "vectors = [word_vectors[word] for word in words]\n",
        "# Reduce dimensions to 2D using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42,perplexity=5)\n",
        "vectors_2d = tsne.fit_transform(np.array(vectors))\n",
        "\n",
        "# Plot the embeddings\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i, word in enumerate(words):\n",
        "    plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1])\n",
        "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), fontsize=12)\n",
        "plt.title(\"Word Embeddings Visualization (GloVe)\", fontsize=14)\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QIq9ZcV9KACd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contextual Embeddings\n",
        "Contextual embeddings, like those from BERT, provide vectors that change based on a word’s context. Unlike static word embeddings, BERT considers the surrounding words, making it ideal for disambiguating meanings (e.g., \"bank\" as a financial institution vs. a riverbank)."
      ],
      "metadata": {
        "id": "3vIdnDv2Nzv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "5ZXBBg3BOuAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "4VCEWjovN9a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAOEXkTgJOOv"
      },
      "outputs": [],
      "source": [
        "# --- Contextual Embeddings with BERT ---\n",
        "sentences_context = [\n",
        "    \"I went to the bank to deposit money.\",\n",
        "    \"The river bank was flooded.\"\n",
        "]\n",
        "bank_embeddings = []\n",
        "for sentence in sentences_context:\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    bank_index = tokens.index('bank')\n",
        "    bank_embedding = embeddings[0, bank_index, :].detach().numpy()\n",
        "    bank_embeddings.append(bank_embedding)\n",
        "    print(f\"Embedding for 'bank' in '{sentence}': {bank_embedding[:5]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = cosine_similarity([bank_embeddings[0]], [bank_embeddings[1]])[0][0]\n",
        "print(f\"Similarity between 'bank' in different contexts: {similarity:.4f}\")"
      ],
      "metadata": {
        "id": "A7Ke843sz8_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the contextual sentences\n",
        "inputs = [tokenizer(sentence, return_tensors='pt') for sentence in sentences_context]\n",
        "\n",
        "# Get BERT outputs (embeddings)\n",
        "with torch.no_grad():\n",
        "    outputs = [model(**input) for input in inputs]\n",
        "\n",
        "# Extract embeddings for specific words\n",
        "bank1 = outputs[0].last_hidden_state[0, 4, :].numpy()  # \"bank\" in \"I went to the bank to deposit money\"\n",
        "money = outputs[0].last_hidden_state[0, 7, :].numpy()  # \"money\" in the same sentence\n",
        "bank2 = outputs[1].last_hidden_state[0, 2, :].numpy()  # \"bank\" in \"The river bank was flooded\"\n",
        "river = outputs[1].last_hidden_state[0, 1, :].numpy()  # \"river\" in the same sentence\n",
        "\n",
        "# Compute similarities\n",
        "similarity_bank1_money = cosine_similarity([bank1], [money])[0][0]\n",
        "similarity_bank2_river = cosine_similarity([bank2], [river])[0][0]\n",
        "similarity_bank1_bank2 = cosine_similarity([bank1], [bank2])[0][0]\n",
        "\n",
        "print(f\"Similarity between 'bank' (finance) and 'money': {similarity_bank1_money:.4f}\")\n",
        "print(f\"Similarity between 'bank' (river) and 'river': {similarity_bank2_river:.4f}\")\n",
        "print(f\"Similarity between 'bank' (finance) and 'bank' (river): {similarity_bank1_bank2:.4f}\")"
      ],
      "metadata": {
        "id": "VvUStqvXN-37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The similarity scores should show that \"bank\" in the financial context is closer to \"money,\" while \"bank\" in the river context is closer to \"river.\" The two \"bank\" embeddings will be less similar, highlighting BERT’s context sensitivity.\n",
        "\n",
        "The plot should position \"bank (finance)\" near \"money\" and \"bank (river)\" near \"river,\" visually demonstrating how BERT adapts embeddings to context.\n",
        "\n",
        "Contextual embeddings are the backbone of state-of-the-art NLP models (e.g., chatbots, question answering), as they handle polysemy and nuanced meanings effectively."
      ],
      "metadata": {
        "id": "R59g-lA0OnQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare embeddings and labels for visualization\n",
        "embeddings_to_plot = np.array([bank1, money, bank2, river])\n",
        "labels = [\"bank (finance)\", \"money\", \"bank (river)\", \"river\"]\n",
        "\n",
        "# Reduce dimensions to 2D using t-SNE\n",
        "tsne_context = TSNE(n_components=2, random_state=42,perplexity=3)\n",
        "embeddings_2d = tsne_context.fit_transform(embeddings_to_plot)\n",
        "\n",
        "# Plot the embeddings\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i, label in enumerate(labels):\n",
        "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1])\n",
        "    plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=12)\n",
        "plt.title(\"Contextual Embeddings Visualization (BERT)\", fontsize=14)\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WM8T_f1lOnqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentence Embeddings\n",
        "Sentence embeddings represent entire sentences as single vectors, capturing their overall meaning. While averaging word embeddings is a simple approach, advanced models like those from sentence-transformers (built on transformer architectures) perform better by considering word interactions.\n",
        "\n"
      ],
      "metadata": {
        "id": "3vBZZjGVPg-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load a pre-trained sentence-transformer model\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "HMWXh4-wQV36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"Natural language processing is a subfield of linguistics.\",\n",
        "    \"It involves computer science and artificial intelligence.\",\n",
        "    \"The goal is to enable computers to understand human language.\",\n",
        "    \"Machine learning is a key component of NLP.\",\n",
        "    \"Deep learning has revolutionized the field.\",\n",
        "    \"Transformers are a type of deep learning model.\",\n",
        "    \"BERT is a popular transformer model for NLP tasks.\"\n",
        "]"
      ],
      "metadata": {
        "id": "wpKEcoAbR0Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for the sentences\n",
        "sentence_embeddings = sentence_model.encode(sentences)\n",
        "\n",
        "# Compute similarity between the first two sentences\n",
        "similarity = cosine_similarity([sentence_embeddings[0]], [sentence_embeddings[1]])[0][0]\n",
        "print(f\"Similarity between sentence 1 and sentence 2: {similarity:.4f}\")"
      ],
      "metadata": {
        "id": "lcrup6PNPvQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce dimensions to 2D using t-SNE\n",
        "tsne_sentences = TSNE(n_components=2, random_state=42,perplexity=3)\n",
        "sentence_vectors_2d = tsne_sentences.fit_transform(sentence_embeddings)\n",
        "\n",
        "# Plot the embeddings\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, sentence in enumerate(sentences):\n",
        "    plt.scatter(sentence_vectors_2d[i, 0], sentence_vectors_2d[i, 1])\n",
        "    plt.annotate(f\"S{i+1}\", (sentence_vectors_2d[i, 0], sentence_vectors_2d[i, 1]), fontsize=10)\n",
        "plt.title(\"Sentence Embeddings Visualization (Sentence-Transformers)\", fontsize=14)\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.show()\n",
        "\n",
        "# Optional: Print sentences with labels for reference\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"S{i+1}: {sentence}\")"
      ],
      "metadata": {
        "id": "es37JHGNPxpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentence Transformers\n",
        "Sentence Transformers are models that produce embeddings (numerical representations) for entire sentences or paragraphs, rather than just individual words. They’re built on transformer models, a type of neural network architecture widely used in natural language processing (NLP). Unlike word embeddings, which give each word a static vector regardless of its context, Sentence Transformers create dynamic, context-aware representations by considering all the words in a sentence together.\n",
        "\n",
        "##How Sentence Transformers Work\n",
        "\n",
        "###Input Encoding: Turning Words into Numbers\n",
        "Tokenization: The sentence is first split into smaller pieces called tokens. These could be whole words (e.g., \"cat\") or subwords (e.g., \"play\" and \"##ing\" from \"playing\"), depending on the model. For example, \"I love NLP\" might become [\"I\", \"love\", \"NLP\"].\n",
        "\n",
        "Embedding Layer: Each token is mapped to an initial numerical vector using an embedding layer. This is similar to word embeddings, but these vectors are just a starting point—they’ll be refined later.\n",
        "\n",
        "###Transformer Layers: Adding Context\n",
        "\n",
        "The magic of Sentence Transformers lies in the transformer architecture, which processes the entire sentence at once. Here’s how it works:\n",
        "\n",
        "* Self-Attention Mechanism: Each token’s representation is updated based on its relationship with every other token in the sentence. For instance, in \"The bank by the river,\" the word \"bank\" gets influenced by \"river,\" helping the model understand it’s a riverbank, not a financial institution.\n",
        "* Multiple Layers: The tokens pass through several transformer layers (e.g., 6 or 12, depending on the model). Each layer refines the embeddings, capturing deeper patterns—early layers might focus on syntax (word order), while later layers grasp semantics (meaning).\n",
        "* Result: After this step, you get a set of contextualized token embeddings, one for each token, where each embedding reflects the token’s meaning in the context of the full sentence.\n",
        "\n",
        "###Pooling: Creating a Single Sentence Embedding\n",
        "The transformer layers give us embeddings for each token, but we want one vector for the whole sentence. This is done through pooling:\n",
        "\n",
        "* Mean Pooling: Take the average of all token embeddings. This is common because it’s simple and captures the overall meaning well.\n",
        "* Max Pooling: Use the maximum value across each dimension of the token embeddings to highlight key features.\n",
        "* CLS Token: Some models (like BERT) add a special [CLS] token at the start of the sentence, and its final embedding represents the sentence. Sentence Transformers often tweak this approach, but mean pooling is a popular default.\n",
        "For example, if \"I love NLP\" produces three token embeddings, mean pooling averages them into one vector that represents the entire sentence.\n",
        "\n",
        "###Fine-Tuning: Sharpening Semantic Understanding\n",
        "To make the embeddings even better, Sentence Transformers are often fine-tuned on specific tasks:\n",
        "\n",
        "* Natural Language Inference (NLI): The model learns to predict if one sentence entails, contradicts, or is neutral relative to another (e.g., \"I love NLP\" vs. \"NLP is great\"). This teaches it to align similar meanings in vector space.\n",
        "Semantic Textual Similarity (STS): The model is trained to score how similar two sentences are, refining its ability to capture nuanced meanings.\n",
        "This fine-tuning ensures the embeddings are optimized for practical tasks like finding similar sentences or classifying text.\n",
        "\n",
        "###Output: A Fixed-Size Sentence Embedding\n",
        "The end result is a single vector (e.g., 384 or 768 dimensions) that encapsulates the sentence’s meaning. This vector can be used for:\n",
        "\n",
        "* Semantic Search: Finding sentences with similar meanings.\n",
        "* Clustering: Grouping related sentences.\n",
        "* Classification: Feeding the vector into a classifier (e.g., for sentiment analysis)."
      ],
      "metadata": {
        "id": "Qq2LdtO5P2dE"
      }
    }
  ]
}