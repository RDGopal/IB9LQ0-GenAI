{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNZSe1R2U0yO6xAnLNmDhp/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9LQ0-GenAI/blob/main/Google_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Google Gemini Models\n",
        "\n",
        "Prerequisites: You need an API key from [Google AI Studio](https://aistudio.google.com/apikey). Everything can be done on the free tier.\n",
        "\n",
        "Acknowledgement: [Patrickloeber](https://github.com/patrickloeber/workshop-build-with-gemini/blob/main/README.md)"
      ],
      "metadata": {
        "id": "HuaFDub9hL4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "e42GveKjYjsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U google-genai"
      ],
      "metadata": {
        "id": "dxFuheTJhsgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('Google_API')"
      ],
      "metadata": {
        "id": "yD1YGQifhjKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "cvLSPKVPhzoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models available: [Models](https://ai.google.dev/gemini-api/docs/models)"
      ],
      "metadata": {
        "id": "wEgG8_dNh1oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gemini-2.0-flash\""
      ],
      "metadata": {
        "id": "wzCHXyG-iG9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "oc-0rkLmAZng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting\n"
      ],
      "metadata": {
        "id": "JaQN8HSNiOOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Tell me three funny dad jokes\"\n",
        ")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "gcx9KF8biWAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##List of Prompts"
      ],
      "metadata": {
        "id": "dtXLtdp6i_mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=[\"Find three best Chinese restaurants\",\"city=Birmingham\"]\n",
        ")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "t1efVMeDjEAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Streaming response\n",
        "By default, the model returns a response after completing the entire text generation process. You can achieve faster interactions by using streaming to return outputs as they are generated."
      ],
      "metadata": {
        "id": "1KwnVVZRkLZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content_stream(\n",
        "    model=MODEL,\n",
        "    contents=[\"Explain how Variational Autoencoders Work\"]\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    display(Markdown(chunk.text))"
      ],
      "metadata": {
        "id": "NLbjEXa9kN4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parameters\n",
        "\n",
        "Every prompt you send to the model includes parameters that control how the model generates responses. You can configure these parameters, or let the model use the default options.\n",
        "\n",
        "* `max_output_tokens`: Sets the maximum number of tokens to include in a candidate.\n",
        "* `temperature`: Controls the randomness of the output. Use higher values for more creative responses, and lower values for more deterministic responses. Values can range from [0.0, 2.0].\n",
        "* `top_p`: Changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the top_p value.\n",
        "^ `top_k`: Changes how the model selects tokens for output. A top_k of 1 means the selected token is the most probable among all the tokens in the model's vocabulary, while a top_k of 3 means that the next token is selected from among the 3 most probable using the temperature. Tokens are further filtered based on top_p with the final token selected using temperature sampling.\n",
        "* `stop_sequences`: List of strings (up to 5) that tells the model to stop generating text if one of the strings is encountered in the response. If specified, the API will stop at the first appearance of a stop sequence.\n",
        "* `seed`: If specified, the model makes a best effort to provide the same response for repeated requests. By default, a random number is used."
      ],
      "metadata": {
        "id": "VfwYy8NmkzMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=[\"Explain Gaussian Splatting\"],\n",
        "    config=types.GenerateContentConfig(\n",
        "        max_output_tokens=100,\n",
        "        temperature=1.0,\n",
        "        top_p=0.95,\n",
        "        top_k=40,\n",
        "        stop_sequences=None,\n",
        "        seed=1234,\n",
        "    )\n",
        ")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "nF2yAVjBlJMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Long context and token counting\n",
        "\n",
        "Gemini 2.0 Flash and 2.5 Pro have a 1M token context window.\n",
        "\n",
        "In practice, 1 million tokens could look like:\n",
        "\n",
        "50,000 lines of code (with the standard 80 characters per line)\n",
        "All the text messages you have sent in the last 5 years\n",
        "8 average length English novels\n",
        "1 hour of video data\n",
        "Let's feed in an entire book and ask questions:"
      ],
      "metadata": {
        "id": "hKU_2pGNlsJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "res = requests.get(\"https://gutenberg.org/cache/epub/16317/pg16317.txt\")\n",
        "book = res.text"
      ],
      "metadata": {
        "id": "4WuRthU8luZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(book[:300]))"
      ],
      "metadata": {
        "id": "VrLO79vfly19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(f\"# characters {len(book)}\"))\n",
        "display(Markdown(f\"# words {len(book.split())}\"))\n",
        "display(Markdown(f\"# tokens: ~{int(len(book.split()) * 4/3)}\"))\n"
      ],
      "metadata": {
        "id": "iydRX55dl5ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Summarize the book.\n",
        "\n",
        "Book:\n",
        "{book}\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "eymDp7PzmDYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the token usage, you can check usage_metadata:"
      ],
      "metadata": {
        "id": "w0WLJxZzmXL9"
      }
    },
    {
      "source": [
        "display(Markdown(f\"{response.usage_metadata.candidates_token_count}\"))  # output\n",
        "display(Markdown(f\"{response.usage_metadata.prompt_token_count}\"))   # input\n",
        "display(Markdown(f\"{response.usage_metadata.total_token_count}\"))   # total"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "W77k-JKMt_dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use count_tokens to check the size of your input prompt(s):"
      ],
      "metadata": {
        "id": "GtviBRDBmnss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = client.models.count_tokens(model=MODEL, contents=prompt)\n",
        "display(Markdown(f\"{res}\"))"
      ],
      "metadata": {
        "id": "ZQxIZa5VmpAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chat with a book !!\n",
        "\n",
        "\n",
        "Create a chat\n",
        "Use a system prompt: \"You are an expert book reviewer with a witty tone.\"\n",
        "Use a temperature of 1.5\n",
        "Ask 1 to summarize the book\n",
        "Ask 1 question to explain more detail about a certain topic from the book\n",
        "Ask to create a social media post based on the book\n",
        "Print the total number of tokens used during the chat"
      ],
      "metadata": {
        "id": "WcOt2PQ2m9XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(\n",
        "    model=MODEL,\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=\"You are an expert book reviewer with a smart and funny tone.\",\n",
        "        temperature=1.5\n",
        "    )\n",
        ")\n",
        "\n",
        "prompt = f\"\"\"Summarize the book in 10 bullet points.\n",
        "\n",
        "Book:\n",
        "{book}\n",
        "\"\"\"\n",
        "\n",
        "response = chat.send_message(prompt)\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "LQCC8MSenAVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"Create a linkedin post with 1 or 2 key insighs from the book. Keep the tone casual and make it inspirational\")\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "rdnTVE7jnN-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multimodality\n",
        "\n"
      ],
      "metadata": {
        "id": "VHG9ImpmFQ6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image understanding\n",
        "Gemini models are able to process and understand images. You can prompt Gemini to describe, caption, and answer questions about images, and use it for object detection."
      ],
      "metadata": {
        "id": "etO2Vy8HFbmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o image.jpg \"https://storage.googleapis.com/generativeai-downloads/images/Cupcakes.jpg\""
      ],
      "metadata": {
        "id": "09x8T8wjFp4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "image = Image.open(\"image.jpg\")\n",
        "print(image.size)\n",
        "image"
      ],
      "metadata": {
        "id": "fUScgSsXFwSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=[\"What is this image?\", image])\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "oYPeXJTOF4tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Your turn\n",
        "Tell Gemini to describe the image\n",
        "Then asked Gemini for a recipe to bake this item. Include item names and quantities for the recipe."
      ],
      "metadata": {
        "id": "-qPF5_8AGN1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o croissant.jpg \"https://storage.googleapis.com/generativeai-downloads/images/croissant.jpg\"\n",
        "image2 = Image.open(\"croissant.jpg\")\n",
        "image2"
      ],
      "metadata": {
        "id": "7algiJ9tGOrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Video\n",
        "Gemini models are able to process videos. The 1M context window support up to approximately an hour of video data.\n",
        "\n",
        "The Gemini API and AI Studio support YouTube URLs as a file data Part. You can include a YouTube URL with a prompt asking the model to summarize, translate, or otherwise interact with the video content."
      ],
      "metadata": {
        "id": "_HI3BrvJG9GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_url = \"https://youtu.be/LlWDx0LSDok\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=types.Content(\n",
        "        parts=[\n",
        "            types.Part(text='Can you summarize this video?'),\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri=youtube_url)\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "U2PX5Z7jG_YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Audio\n",
        "\n",
        "You can use Gemini to process audio files. For example, you can use it to generate a transcript of an audio file or to summarize the content of an audio file.\n",
        "\n",
        "Gemini represents each second of audio as 32 tokens; for example, one minute of audio is represented as 1,920 tokens."
      ],
      "metadata": {
        "id": "xougW0QAHklX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://storage.googleapis.com/generativeai-downloads/data/jeff-dean-presentation.mp3\"\n",
        "!wget -q $URL -O sample.mp3"
      ],
      "metadata": {
        "id": "AOwY8FMLHo3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "IPython.display.Audio(\"sample.mp3\")"
      ],
      "metadata": {
        "id": "P6KYtT-pHvE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_audio_file = client.files.upload(file='sample.mp3')\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=MODEL,\n",
        "  contents=[\n",
        "    'Here is a talk by Jeff Dean. Summarize the talk in 4-5 sentences.',\n",
        "    uploaded_audio_file,\n",
        "  ]\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "-6eJFzA-H8z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Structured output\n",
        "Gemini generates unstructured text by default, but some applications require structured text. For these use cases, you can constrain Gemini to respond with JSON, a structured data format suitable for automated processing."
      ],
      "metadata": {
        "id": "ELUsLN3LLCcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "  recipe_name: str\n",
        "  ingredients: list[str]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents='List a three popular cookie recipes. Be sure to include the amounts of ingredients.',\n",
        "    config={\n",
        "        'response_mime_type': 'application/json',\n",
        "        'response_schema': list[Recipe],\n",
        "    },\n",
        ")\n",
        "# Use the response as a JSON string.\n",
        "print(response.text)\n",
        "# Use instantiated objects.\n",
        "my_recipes: list[Recipe] = response.parsed"
      ],
      "metadata": {
        "id": "nsqquyH4LC1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Grounding with Google Search\n",
        "If Google Search is configured as a tool, Gemini can decide when to use Google Search to improve the accuracy and recency of responses."
      ],
      "metadata": {
        "id": "mD-EnMrWOwb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai.types import Tool, GenerateContentConfig, GoogleSearch\n",
        "\n",
        "google_search_tool = Tool(\n",
        "    google_search = GoogleSearch()\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"What is the big news item today?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[google_search_tool],\n",
        "        response_modalities=[\"TEXT\"],\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "fODmyTPaOxyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "    display(Markdown(part.text))"
      ],
      "metadata": {
        "id": "u3B99ykGPOi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function Calling\n",
        "Function calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions."
      ],
      "metadata": {
        "id": "NUYV6fTeQRW2"
      }
    },
    {
      "source": [
        "from google.genai import types\n",
        "\n",
        "# Define the function declaration for the model\n",
        "def add_numbers(a, b):\n",
        "  \"\"\"Adds two numbers together.\"\"\"\n",
        "  return a + b\n",
        "\n",
        "add_numbers_function = {\n",
        "    \"name\": \"add_numbers\",\n",
        "    \"description\": \"Adds two numbers together.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"a\": {\n",
        "                \"type\": \"number\",\n",
        "                \"description\": \"The first number\",\n",
        "            },\n",
        "            \"b\": {\n",
        "                \"type\": \"number\",\n",
        "                \"description\": \"The second number\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"a\", \"b\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Configure the client and tools\n",
        "tools = types.Tool(function_declarations=[add_numbers_function])\n",
        "\n",
        "# Send request with function declarations\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"What is the sum of 300 and 5?\",\n",
        "    config=types.GenerateContentConfig(tools=[tools])\n",
        ")\n",
        "\n",
        "# Check if the response contains a function call\n",
        "if response.candidates[0].content.parts[0].function_call:\n",
        "    function_call = response.candidates[0].content.parts[0].function_call\n",
        "    print(f\"Function to call: {function_call.name}\")\n",
        "    print(f\"Arguments: {function_call.args}\")\n",
        "\n",
        "    # Call the function with the provided arguments\n",
        "    result = add_numbers(**function_call.args)  # Unpack arguments\n",
        "    print(f\"Result: {result}\")\n",
        "else:\n",
        "    print(\"No function call found in the response.\")\n",
        "    print(response.text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BiLgITwSUSHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "from google.genai import types\n",
        "import io # import the io module\n",
        "\n",
        "# Define a function to analyze data\n",
        "def analyze_data(data: str, column: str, operation: str) -> dict:\n",
        "  \"\"\"Analyzes the provided data using pandas.\"\"\"\n",
        "\n",
        "  # Convert the data string to a DataFrame using io.StringIO\n",
        "  df = pd.read_csv(io.StringIO(data))\n",
        "\n",
        "  # Perform the requested operation\n",
        "  if operation == \"mean\":\n",
        "    result = df[column].mean()\n",
        "  elif operation == \"median\":\n",
        "    result = df[column].median()\n",
        "  elif operation == \"sum\":\n",
        "    result = df[column].sum()\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid operation: {operation}\")\n",
        "\n",
        "  return {\"result\": result}\n",
        "\n",
        "# Create a function declaration\n",
        "analyze_data_function = {\n",
        "    \"name\": \"analyze_data\",\n",
        "    \"description\": \"Analyzes data using pandas. Accepts a CSV string, column name, and operation (mean, median, sum).\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"data\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The data to analyze in CSV format\",\n",
        "            },\n",
        "            \"column\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The name of the column to analyze\",\n",
        "            },\n",
        "            \"operation\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The operation to perform (mean, median, sum)\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"data\", \"column\", \"operation\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Configure the tools\n",
        "tools = types.Tool(function_declarations=[analyze_data_function])\n",
        "\n",
        "# Sample data\n",
        "data = \"\"\"\n",
        "col1,col2,col3,col4\n",
        "1,2,3,4\n",
        "5,6,7,8\n",
        "9,10,11,12\n",
        "1,5,9,8\n",
        "\"\"\"\n",
        "\n",
        "# Send the request\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=f\"Calculate the median of the 'col3' column in this data:\\n\\n{data}\",\n",
        "    config=types.GenerateContentConfig(tools=[tools])\n",
        ")\n",
        "\n",
        "# Check for function call and process the response\n",
        "if response.candidates[0].content.parts[0].function_call:\n",
        "  function_call = response.candidates[0].content.parts[0].function_call\n",
        "  print(f\"Function to call: {function_call.name}\")\n",
        "  print(f\"Arguments: {function_call.args}\")\n",
        "\n",
        "  result = analyze_data(**function_call.args)\n",
        "  print(f\"Result: {result}\")\n",
        "else:\n",
        "  print(\"No function call found in the response.\")\n",
        "  print(response.text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1fI4wv6VWDg6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}