{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMGT721zsDPo4YsCuN+ESzv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9LQ0-GenAI/blob/main/8_1_SQL_based_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SQL-based Retrieval Augemented Generation\n",
        "In this tutorial we will be query a relational database to answer user queries. The contribution here is that rather than a user having to specify the query as a SQL-string (which also means they will need to understand the structure of the database), they can express their query as natural language. E.g.\n",
        "\n",
        "_\"How many students of MSBA gave 4 stars or higher on the module review?\"_\n",
        "\n",
        "Might be converted to:\n",
        "\n",
        "```\n",
        "SELECT COUNT(DISTINCT student_id)\n",
        "FROM reviews\n",
        "WHERE course = 'MSBA' AND AVG(review_score) >= 4;\n",
        "```\n",
        "We can start by setting up the instance.\n",
        "\n",
        "You will need to use a Hugging Face token to make this work. Follow these steps:\n",
        "1. Got to https://huggingface.co/\n",
        "2. Click \"Sign Up\" in the top right corner.\n",
        "3. Do the usual account sign up steps.\n",
        "4. Make sure you go to your email and click on the link to confirm your account.\n",
        "5. Once logged-in, click on your icon in the top right corner and select \"Access tokens\" (right at the bottom of the menu).\n",
        "6. Click \"+ Create new token\".\n",
        "7. Give your token a name and then scroll to the bottom to click \"Create\". You can ignore all the other options.\n",
        "8. Copy your token secret (\"hf_...\") and save it somewhere on your machine (e.g. Word or Notepad).\n",
        "9. Back in Colab, click on the key icon on the left hand side.\n",
        "10. Click on \"+ Add new secret\".\n",
        "11. Give the new secret the Name HF_TOKEN (please copy exactly this name).\n",
        "12. Paste in your token secret from step 8 as the Value.\n",
        "13. Make sure Notebook access is slid to the right. If done it will go blue and show a tick.\n",
        "14. Read on!"
      ],
      "metadata": {
        "id": "OSxMEcJC3Dsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q faker sqlite-utils transformers accelerate\n",
        "\n",
        "# Retrieve Hugging Face API token from Colab Secrets\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_api_token = userdata.get(\"HF_TOKEN\")  # Set this secret via Runtime > Secrets\n",
        "\n",
        "if hf_api_token:\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_api_token\n",
        "else:\n",
        "    raise ValueError(\"Hugging Face API Token is missing. Please set it in Colab Secrets.\")"
      ],
      "metadata": {
        "id": "HYDJ7a662Hy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the above code worked you have added your token! This will be available from all Colab Notebooks (including future ones in these classes), although you need to follow option 13 (slide to the right to get the blue tick) to make it avaialble each time.\n",
        "\n",
        "Now we will set up the model:"
      ],
      "metadata": {
        "id": "VHDK2x5QJ-hN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Falcon-7B-Instruct\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_api_token)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    token=hf_api_token\n",
        ")"
      ],
      "metadata": {
        "id": "MK3RCNaK2IdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much of the above should look familiar to you from ADA. Here we are using a fairly small open-source model (Falcon with 7bil parameters ... which would be considered huge five years ago but there you go).\n",
        "\n",
        "As with any transformer architecture, we need to tokenise our data (in this case the user queries that will be translated to SQL). We will set the tokeniser to be the same as our main model (Falcon 7b).\n",
        "\n",
        "Lastly we download the model, setting some parameters such as the use of \"auto\" to map tasks to our machine (basically to move stuff onto the GPU or CPU), the size of our tensor floats (\"bfloat16\"), that we need to trust the code associated with the model (\"trust_remote_code=True\") and that we use our HuggingFace token we loaded as a secret earlier.\n",
        "\n",
        "Next we will build a SQL database:"
      ],
      "metadata": {
        "id": "NZGjPTgbLcpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a fake SQLite database\n",
        "import sqlite3\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker()\n",
        "conn = sqlite3.connect(\"people.db\")\n",
        "cur = conn.cursor()\n",
        "\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS people (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    name TEXT,\n",
        "    email TEXT,\n",
        "    job TEXT,\n",
        "    city TEXT,\n",
        "    age INTEGER\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Add 100 fake rows\n",
        "for _ in range(100):\n",
        "    cur.execute(\"INSERT INTO people (name, email, job, city, age) VALUES (?, ?, ?, ?, ?)\", (\n",
        "        fake.name(), fake.email(), fake.job(), fake.city(), fake.random_int(min=20, max=70)\n",
        "    ))\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "wiD40Wkq2L3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This one should be familiar to you from Data Management. We're simply making a SQLite table and populating ti with data (using Faker).\n",
        "\n",
        "Next we will build our function to create SQL from natural language (via the LLM):"
      ],
      "metadata": {
        "id": "w7_sGbEOMsXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate SQL from natural language\n",
        "def generate_sql(nl_query): # pass a natural language query to the function (nl_query)\n",
        "\n",
        "    # prompt template\n",
        "    prompt = f\"\"\"\n",
        "      You are an expert SQL generator.\n",
        "      Translate the user's natural language question into an SQLite query.\n",
        "      Important! Return only the SQL code and nothing else.\n",
        "\n",
        "      Schema:\n",
        "      people(id, name, email, job, city, age)\n",
        "\n",
        "      User question: {nl_query}\n",
        "      Important! Return only the SQL code and nothing else.\n",
        "      SQL query:\n",
        "    \"\"\"\n",
        "\n",
        "    # tokenise the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # generate outputs\n",
        "    # **inputs means upack all the inputs (tokens)\n",
        "    # max_new_tokens is the maximum number of tokens to be generated (150)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=150)\n",
        "\n",
        "    # decode from tokens back into language (SQL).\n",
        "    # ignore special tokens such as \"BOS\" and \"SEP\"\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # extract the last part of the response (ignore the prompt, etc.)\n",
        "    # this assumes that the LLM writes the prompt directly after\n",
        "    # \"SQL query:\"\n",
        "    sql = generated.split(\"SQL query:\")[-1].strip().split(\"\\n\")[0]\n",
        "\n",
        "    # quite possible the function won't work because the LLM doesn't return the right info\n",
        "    # you can mess around to get the output you need e.g.\n",
        "\n",
        "    sql = generated.split(\"SQL query:\")[-1].strip().split(\"\\n\")[1].replace(\"`\", \"\").replace(\"`\", \"\").replace(\"  \",\"\")\n",
        "\n",
        "    # however, probably just better to use a better model\n",
        "\n",
        "\n",
        "    return sql"
      ],
      "metadata": {
        "id": "7q8Bzlja2UB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the \"sql=\" bit:\n",
        "\n",
        "```\n",
        "sql = generated.split(\"SQL query:\")[-1].strip().split(\"\\n\")[0]\n",
        "```\n",
        "\n",
        "Firstly we are splitting the generated output effectively breaking the output into two. The first part is anything before the \"SQL query:\" (i.e. the end of the prompt. The second part is everything after the prompt. The _[-1]_ part means we keep just the second part (drop the prompt).\n",
        "\n",
        "The next part ... _strip()_ ... means we get rid of any trailing spaces. E.g. \"   Hello, World!   \" would become \"Hello, World!\".\n",
        "\n",
        "_.split(\"\\n\")_, the next part of the code, splits the remaining outputs into separate chunks for each line break (\"\\n\") in the text. This _should_ mean we get the output SQL code as the first item, and probably some further text generated by the LLM as separate items after that. We finally use _[0]_ to return just the SQL code.\n",
        "\n",
        "Of course, this assumes it all works properly but with a smaller model (7bil paramaters) this is not guaranteed. I had to mess around a bit taken different items and replacing different characters such as \" ` \" (back tick) from the output. Your results may vary and you can try to fix them (based on the SQL generated output), or otherwise do not worry. With a bigger model this shouldn't happen.\n",
        "\n",
        "Now that we have a prompt template for the LLM, we need a function to wrap around it to process the natural language query and feed the SQL output into the database:"
      ],
      "metadata": {
        "id": "uox4Wi6N7aXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query executor\n",
        "def run_query(nl_query):\n",
        "\n",
        "    # call the generate_sql function with the user query (natural language)\n",
        "    sql = generate_sql(nl_query)\n",
        "    # print the output\n",
        "    print(\"SQL Generated:\", sql)\n",
        "\n",
        "    # use try in case the SQL is bad\n",
        "    try:\n",
        "        # run the query on the database\n",
        "        result = cur.execute(sql).fetchall()\n",
        "\n",
        "        # if it works print the first 5 results\n",
        "        if result:\n",
        "            print(\"\\n Top 5 Results:\")\n",
        "            for row in result[:5]:\n",
        "                print(row)\n",
        "        else:\n",
        "            print(\"No results found.\")\n",
        "\n",
        "    # print database error if the try fails\n",
        "    except Exception as e:\n",
        "        print(\"SQL Error:\", e)"
      ],
      "metadata": {
        "id": "weTGkty52cj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now with our functions generated, we can test if it works!"
      ],
      "metadata": {
        "id": "ngrpQlFb7ngf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the full pipeline\n",
        "run_query(\"List the names and jobs of people over 50\")"
      ],
      "metadata": {
        "id": "wFR3SSQ_2gr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a play around and see if you can write more complex queries. How far can you push the 7B model?"
      ],
      "metadata": {
        "id": "Ma3pxTrbJmKz"
      }
    }
  ]
}