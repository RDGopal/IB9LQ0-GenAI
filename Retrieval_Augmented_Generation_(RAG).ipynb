{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMPX2cxvNtKXCUDTry4TplD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9LQ0-GenAI/blob/main/Retrieval_Augmented_Generation_(RAG).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval Augmented Generation (RAG)\n",
        "LLMs typically have the following limitations:\n",
        "\n",
        "1. Knowledge Cutoff: Their knowledge is limited to the data they were trained on (often months or years old).\n",
        "2. Hallucinations: They can confidently make up facts or generate plausible-sounding but incorrect information.\n",
        "3. Lack of Domain-Specific/Private Knowledge: They don't know the content of your organization's information.\n",
        "\n",
        "RAG is a technique to overcome these limitations by giving the LLM access to external, specific, and up-to-date information at the time of answering. The RAG setup consists of the following key steps.\n",
        "\n",
        "1. Document Loading:\n",
        "2. Text Splitting: Why split? Documents are long, LLM context windows are limited. We need to break large documents into smaller, manageable chunks.\n",
        "3. Embedding: How do we find relevant chunks quickly? We convert text chunks into embeddings which capture the semantic meaning, so chunks with similar meaning have similar vectors.\n",
        "4. Vector Storage: Where do we store the vectors for fast search? A Vector Database allows searching for vectors similar to a query vector very quickly.\n",
        "5. Query Embedding: When a user asks a question, convert the query text into a vector using the same embedding model used for the documents.\n",
        "6. Retrieval: Use the query vector to search the Vector Store for the most similar document vectors. Retrieve the top K (e.g., 2-5) corresponding text chunks.\n",
        "7. Context Stuffing: Take the original user query and the retrieved text chunks. Combine them into a single prompt for the LLM. The prompt will look something like: \"Here is some context: [Retrieved Chunk 1] [Retrieved Chunk 2] ... Based on this context, answer the following question: [User Query]\".\n",
        "8. Answer Generation: The LLM receives the prompt containing the specific context. It generates an answer based on and limited by the provided context. This significantly reduces hallucinations and ensures the answer is relevant to the external data."
      ],
      "metadata": {
        "id": "l3PDmvQY5IGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Packages\n",
        "langchain (orchestration framework), pypdf (read PDFs), sentence-transformers (embedding model), ctransformers (run local LLMs), chromadb (vector store)."
      ],
      "metadata": {
        "id": "OdD7dvA37R9E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVzG-DTEuI1n"
      },
      "outputs": [],
      "source": [
        "!pip install langchain pypdf sentence-transformers ctransformers chromadb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Loading & Processing\n",
        "We will create a directory (`docs`) where we will load all the documents."
      ],
      "metadata": {
        "id": "GwIj2I1Uuj96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir docs"
      ],
      "metadata": {
        "id": "8ON2mPOzxQxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now upload all the files to this directory. You can upload them manually or use the following to get the files from the GitHub repository."
      ],
      "metadata": {
        "id": "rMDeYlMnx2ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget -q"
      ],
      "metadata": {
        "id": "6YO63hbq3CFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import os\n",
        "import requests\n",
        "\n",
        "def get_github_files(repo_owner, repo_name, directory_path):\n",
        "  \"\"\"\n",
        "  Fetches a list of PDF files from a GitHub repository directory.\n",
        "\n",
        "  Args:\n",
        "      repo_owner (str): The owner of the GitHub repository.\n",
        "      repo_name (str): The name of the GitHub repository.\n",
        "      directory_path (str): The path to the directory within the repository.\n",
        "\n",
        "  Returns:\n",
        "      list: A list of file URLs for the PDF files in the directory.\n",
        "  \"\"\"\n",
        "  api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{directory_path}\"\n",
        "  headers = {\"Accept\": \"application/vnd.github+json\"}  # For the latest API version\n",
        "  response = requests.get(api_url, headers=headers)\n",
        "  response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "  pdf_files = []\n",
        "  for file_data in response.json():\n",
        "      if file_data[\"type\"] == \"file\" and file_data[\"name\"].endswith(\".pdf\"):\n",
        "          # Use file_data['path'] to construct the correct download URL\n",
        "          # to handle spaces and special characters in file names.\n",
        "          download_url = f\"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/main/{file_data['path']}\"\n",
        "          pdf_files.append(download_url)\n",
        "  return pdf_files\n",
        "\n",
        "# --- Usage ---\n",
        "repo_owner = \"RDGopal\"\n",
        "repo_name = \"IB9LQ0-GenAI\"\n",
        "directory_path = \"Data/Onboarding\"\n",
        "\n",
        "pdf_urls = get_github_files(repo_owner, repo_name, directory_path)\n",
        "\n",
        "# Create the 'docs' directory if it doesn't exist\n",
        "os.makedirs(\"docs\", exist_ok=True)\n",
        "\n",
        "# Download the PDF files\n",
        "for url in pdf_urls:\n",
        "    filename = os.path.basename(url)\n",
        "    wget.download(url, out=os.path.join(\"docs\", filename))\n",
        "    print(f\"Downloaded: {filename}\")"
      ],
      "metadata": {
        "id": "HzUuyDWD5sWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "TYE3Rw8FvFPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "9w9a5iqSunIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read all the files in the directory"
      ],
      "metadata": {
        "id": "N3AFQC-svVuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Assuming PDFs are in a 'docs' folder\n",
        "pdf_folder_path = 'docs/'\n",
        "if not os.path.exists(pdf_folder_path):\n",
        "    print(f\"Error: '{pdf_folder_path}' not found. Please upload your PDFs there.\")\n",
        "else:\n",
        "    loaders = [PyPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path) if fn.endswith('.pdf')]\n",
        "    print(f\"Found {len(loaders)} PDF documents.\")\n",
        "    docs = []\n",
        "    for loader in loaders:\n",
        "        docs.extend(loader.load())\n",
        "    print(f\"Loaded {len(docs)} pages total.\")"
      ],
      "metadata": {
        "id": "_5QnGaWhvWEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the documents into chunks that are overlapping."
      ],
      "metadata": {
        "id": "m8PujZ0yyW7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=25)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(f\"Split into {len(splits)} chunks.\")"
      ],
      "metadata": {
        "id": "k9aWNosqyfWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, split in enumerate(splits):\n",
        "    print(f\"Chunk {i + 1}:\\n{split.page_content}\\n\")"
      ],
      "metadata": {
        "id": "-7vY11dJu23M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Embedding and Indexing"
      ],
      "metadata": {
        "id": "kGj_3HwRyqWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "9JNKE2CqyuCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a sentence embedding model"
      ],
      "metadata": {
        "id": "jXVToUt98JqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "print(\"Embedding model loaded.\")"
      ],
      "metadata": {
        "id": "ebIKGEskyy6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and populate the vector store"
      ],
      "metadata": {
        "id": "jAbt-2XjzIBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"db\"  # Specify the directory for persistence\n",
        "# Create or load the Chroma vector store, enabling persistence to disk.\n",
        "vectorstore = Chroma.from_documents(\n",
        "   documents=splits, embedding=embeddings, persist_directory=persist_directory)\n",
        "vectorstore.persist()\n",
        "print(f\"Vector store created and populated with embeddings, persisted to {persist_directory}.\")\n"
      ],
      "metadata": {
        "id": "l7GswTHy_wRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple Retrieval\n",
        "Let's look at simple retrieval from the vector store"
      ],
      "metadata": {
        "id": "xT2mIT-DzVaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "example_query = \"tell me about drug testing\" #@param {type:\"string\"}\n",
        "retrieved_docs = retriever.invoke(example_query)\n",
        "print(f\"\\nExample Retrieval for query: '{example_query}'\")\n",
        "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "     print(f\"--- Document {i+1} ---\")\n",
        "     print(doc.page_content[:500] + \"...\") # Print first 200 chars\n",
        "     print(f\"Source: {doc.metadata.get('source', 'N/A')}\")"
      ],
      "metadata": {
        "id": "QpjdpeOLzZ0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LLM Setup"
      ],
      "metadata": {
        "id": "qqGYu8xtzyJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import CTransformers\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "iEt4LbREz333"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "llm = CTransformers(\n",
        "    model=\"TheBloke/Llama-2-7B-Chat-GGML\", # Specify a public model file name\n",
        "    model_type=\"llama\", # Specify the model type\n",
        "    config={'max_new_tokens': 1024, 'temperature': 0.1}\n",
        ")\n",
        "print(\"Local LLM model loaded\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LnN88a7i0d2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG chain"
      ],
      "metadata": {
        "id": "5v4o39Ky0rMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "print(\"RetrievalQA chain created.\")"
      ],
      "metadata": {
        "id": "1BeZIG2U0sWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "print(f\"Retriever configured to return top {retriever.search_kwargs['k']} documents.\")"
      ],
      "metadata": {
        "id": "6NqY6iDj3wPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Answer Query"
      ],
      "metadata": {
        "id": "2GnF-ff50733"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"how many holidays do I get?\" #@param {type:\"string\"}\n",
        "print(f\"\\nRunning Query 1: '{query1}'\")\n",
        "response1 = qa_chain.invoke({\"query\": query1})\n",
        "print(\"\\n--- Response  ---\")\n",
        "print(response1['result'])\n",
        "if 'source_documents' in response1:\n",
        "     print(\"\\n--- Sources ---\")\n",
        "     for i, doc in enumerate(response1['source_documents']):\n",
        "         print(f\"Source {i+1}: {doc.metadata.get('source', 'N/A')} (page {doc.metadata.get('page', 'N/A')})\")"
      ],
      "metadata": {
        "id": "KCRHbJNr1JIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Callback Handlers\n",
        "These handlers allow you to hook into various stages of the chain's execution, including when the LLM is invoked and what input it receives."
      ],
      "metadata": {
        "id": "mjytGW9PC59m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import StdOutCallbackHandler\n",
        "\n",
        "# Create an instance of the StdOutCallbackHandler\n",
        "handler = StdOutCallbackHandler()\n",
        "\n",
        "# --- Run your queries, but pass the handler via the 'config' parameter ---\n",
        "\n",
        "print(f\"\\nRunning Query 1: '{query1}' with callback tracing...\")\n",
        "\n",
        "# Use the .invoke() method and pass callbacks in the config dictionary\n",
        "response1 = qa_chain.invoke(\n",
        "    {\"query\": query1},\n",
        "    config={\"callbacks\": [handler]} # <-- Add this config\n",
        ")\n",
        "\n",
        "print(\"\\n--- Final Response 1 ---\")\n",
        "print(response1['result'])\n",
        "if 'source_documents' in response1:\n",
        "     print(\"\\n--- Sources ---\")\n",
        "     for i, doc in enumerate(response1['source_documents']):\n",
        "         print(f\"Source {i+1}: {doc.metadata.get('source', 'N/A')} (page {doc.metadata.get('page', 'N/A')})\")\n",
        "\n",
        "print(\"-\" * 30) # Separator"
      ],
      "metadata": {
        "id": "IFdS9sX9B0P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Gemini LLM"
      ],
      "metadata": {
        "id": "HCfcir5CFF8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai  # Install or update google-genai\n",
        "!pip install -q -U google-generativeai  # Install or update google-generativeai\n",
        "\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "\n",
        "# Set your Google API key (ensure it's stored securely)\n",
        "GOOGLE_API_KEY = userdata.get('Google_API')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "MODEL = \"gemini-2.0-flash\""
      ],
      "metadata": {
        "id": "8dWsDKLEL3cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def answer_with_gemini(query):\n",
        "    \"\"\"\n",
        "    Retrieves semantically similar chunks and uses Gemini to answer the query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's question.\n",
        "\n",
        "    Returns:\n",
        "        str: Gemini's answer to the question.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Retrieval of semantically similar chunks:\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    retrieved_docs = retriever.invoke(query)\n",
        "\n",
        "    # 2. Construct the prompt for Gemini:\n",
        "    context = \"\"\n",
        "    for doc in retrieved_docs:\n",
        "        context += doc.page_content\n",
        "\n",
        "    # System instructions for summarizing and structuring\n",
        "    system_instructions = \"\"\"\n",
        "    You are a helpful and informative AI assistant.\n",
        "    Summarize and structure your response based on the provided context,\n",
        "    specifically addressing the user's query.\n",
        "    If the context does not contain the answer, state that you don't know.\n",
        "    Do not fabricate an answer.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"{system_instructions}\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {query}\n",
        "    \"\"\"\n",
        "\n",
        "    # 3. Generate the answer using Gemini:\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL,\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    return response.text"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ksyXx7p_MC4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage:\n",
        "user_question = \"Tell me about medical insurance\" #@param {type:\"string\"}\n",
        "answer = answer_with_gemini(user_question)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "GTN67dYsM3iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Your turn\n",
        "Visit the link https://warwick.ac.uk/news. Get a few articles and create pdf documents. Build a RAG system based on these documents."
      ],
      "metadata": {
        "id": "h-W6HI0O2Vq-"
      }
    }
  ]
}