{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSLxV0tLHO5eSro3icXEXj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9LQ0-GenAI/blob/main/8_2_simple_agent_with_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Agent with Tools\n",
        "Here we will enhance a simple, LLM-based agent by giving it access to a simple tool ... a calculator. While LLMs have got a lot better at doing calculations, in general they do not understand math and will answer math questions based on probable words (i.e. memorisation) not based on calculating the actual sums. To avoid any hallucinations we will give our agent a simple, Python-base calculator.\n",
        "\n",
        "First, we need to setup the LLM as before:"
      ],
      "metadata": {
        "id": "fmLtDP-ARDK8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASdnalELE4Yq"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate\n",
        "\n",
        "# Load Hugging Face token (stored as a secret in Colab or prompted)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Load Falcon-7B-Instruct model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# If you haven't already added the secret in Colab, here's how:\n",
        "# Runtime > Secrets > Set 'HF_TOKEN' with your Hugging Face API token\n",
        "hf_api_token = userdata.get('HF_TOKEN')  # Make sure the secret is set in Colab\n",
        "\n",
        "# Set the token if it's retrieved correctly, else print an error\n",
        "if hf_api_token:\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_api_token\n",
        "else:\n",
        "    print(\"Hugging Face API Token is missing. Please set it in Colab Secrets.\")\n",
        "\n",
        "model_id = \"tiiuae/falcon-7b-instruct\" # faster but less accurate\n",
        "#model_id = \"tiiuae/Falcon3-10B-Instruct\" # this will be quite slow\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_api_token)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our model we can create our calculator:"
      ],
      "metadata": {
        "id": "zHcoor7IR4h4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define calculator tool\n",
        "def calculator(expression):\n",
        "    try:\n",
        "        return str(eval(expression))\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\""
      ],
      "metadata": {
        "id": "EhKD5g7CQwfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've created a very simple calulator that takes the output of the LLM (which should be an equation) and evaluates it as math. We then can return the results as a string.\n",
        "\n",
        "Now we can define the agent. Note, the agent, if using a calculator, needs to work in three stages:\n",
        "1. First it needs to determine if it needs to use the calculator tool;\n",
        "2. If so, it then needs to pass the equation to the calculator;\n",
        "3. Once it has the result from the calculator it needs to return this answer as part of its prompt.\n",
        "\n",
        "For this reason the code may look quite complicated, but we'll break it down."
      ],
      "metadata": {
        "id": "IMImCzXJR9P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple agent that detects [TOOL: calculator ...] and responds\n",
        "def run_agent(query):\n",
        "    prompt = f\"\"\"You are a helpful assistant. You may use tools.\n",
        "\n",
        "      If a calculation is needed, request it like this:\n",
        "      [TOOL: calculator 5 * (6 + 1)]\n",
        "      Please replace the formula in the example with the actual calculation.\n",
        "      Then use the result in your final answer.\n",
        "\n",
        "      User: {query}\n",
        "      Assistant:\"\"\"\n",
        "\n",
        "    # Run initial LLM to get the first response ...\n",
        "    # this may be a call to the calculator\n",
        "\n",
        "    # first tokenise the query\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # generate the (initial) output from the LLM\n",
        "    outputs = model.generate(**inputs, max_new_tokens=300)\n",
        "\n",
        "    # decode (convert from output tokens to natural language)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"\\n Model Response:\\n\", response)\n",
        "\n",
        "    # Check for tool call\n",
        "    if \"[TOOL: calculator\" in response:\n",
        "        # if the response includes a call to the calculator then\n",
        "        try:\n",
        "            # find the part of the generated text that includes the equation\n",
        "            # see 8.1 Notebook for more discussion of this\n",
        "            tool_call = response.split(\"[TOOL: calculator\", 1)[1].split(\"]\", 1)[0].strip()\n",
        "\n",
        "            # pass the equation to the calulator function\n",
        "            result = calculator(tool_call)\n",
        "            print(f\"\\n Calculator called with: {tool_call} = {result}\")\n",
        "\n",
        "            # Continue with result\n",
        "            # here we make another call to the LLM with the results of the calculator\n",
        "            followup = f\"{response}\\n[RESULT: {result}]\\nAssistant:\"\n",
        "\n",
        "            # again, tokenise input, get the results and decode the results\n",
        "            followup_inputs = tokenizer(followup, return_tensors=\"pt\").to(model.device)\n",
        "            followup_output = model.generate(**followup_inputs, max_new_tokens=100)\n",
        "            final = tokenizer.decode(followup_output[0], skip_special_tokens=True)\n",
        "            print(\"\\n Final Answer:\\n\", final)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Tool error:\", e)\n",
        "\n",
        "    else:\n",
        "        print(\"\\n No tool needed.\")"
      ],
      "metadata": {
        "id": "iq1-4sfeQzY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can test it!"
      ],
      "metadata": {
        "id": "eHk4QmFcRP2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it out\n",
        "run_agent(\"What is (12 + 4) * 3?\")"
      ],
      "metadata": {
        "id": "ukoCpIhvRDet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using the 7bil parameter model (as the code will execute unless you change it) the results are ...\n",
        "\n",
        "... underwhelming.\n",
        "\n",
        "The LLM is too dumb to replace the example equation with the actual equation and so calculates 5 * (6 + 1) rather than (12 + 4) * 3. Inexplicably, in my results it then adds 1 to this before returning as 36. Idiot.\n",
        "\n",
        "If you want to upgrade the model then just uncomment to use the 10bil parameter model and it should work. However, installing and running will take much longer.\n",
        "\n",
        "Regardless though, we have seen how such an approach would work!"
      ],
      "metadata": {
        "id": "5NOxKPA-Tb_t"
      }
    }
  ]
}